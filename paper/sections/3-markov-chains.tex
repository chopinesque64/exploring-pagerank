\documentclass[../exploring-pagerank.tex]{subfiles}

\begin{document}
	\subsection{Stochasticity}
	 Consider a vector $d$, where $d_i=1$ if page $p_i$ dangles and $d_i=0$ otherwise, and let $v$ be a distribution over all pages in $\mathcal{W}$, given as a row vector. Brin and Page used a uniform distribution, but we will be more general. We stochasticize $H$ thusly:
	\begin{equation}
	    \label{eqn:S}
		S=H + d\transpose{v}.
	\end{equation}
	 If a ``random surfer'' on the Web hits a dangling page, it chooses based upon the distribution $v$ what page to next visit. Let $\mathbbm{1}$ represent a vector of ones, and note that the row-stochasticity of $S$ implies $\Vert S \Vert_{\infty} = 1$, i.e. $S \mathbbm{1} = \mathbbm{1}$. Thus, $(1, \mathbbm{1})$ provides a right eigenpair for every stochastic matrix. Now, consider an eigenpair $(v, \lambda)$ of a stochastic matrix $S$ and suppose $|\lambda| > 1$. Then, the vector given by $|\lambda|^n v = S^n v$ grows exponentially, so at least one $(S^n)_{ij} > 1$ for some $n$. This, however, contradicts the stochasticity of $S$. Thus, we conclude that $|\lambda| \leq 1$, and since we have already 1 as an eigenvalue, we conclude the spectral radius $\rho(S)=1$. This result can be proved directly from the Gershgorin circle theorem, but we will not explore that here.
	 
	\subsection{Transition Probability}
	As noted above, we can view $S_{ij}$ as the probability that a random surfer on a Web jumps from page $p_i$ to page $p_j$ at a given time. Thus, we can model our spectral ranking problem as a Markov chain. In this conversation, we consider homogeneous Markov chains -- those that have transition probabilities independent of time. This probabalistic insight leads us to view $S$ as the 1-transition matrix of a finite-state, discrete-time Markov chain. Recall that a discrete-time stochastic process $\{X_t\}_{t \in T}$ for a countable set $T$ over a finite state space $\Omega = \{s_i\}_{i\leq n}$ is a Markov chain if, for each $t$, the Markov property holds:
	\begin{equation*}
		\prob(X_{t+1} = s_j \mid X_t = s_{i_t}, X_{t-1} = s_{i_{t-1}},\cdots, X_0 = s_{i_0}) = \prob(X_{t+1} = s_j \mid X_t=s_{i_{t}}).
	\end{equation*}
	Markov chains are colloquially described by \textit{memorylessness}, which the Markov property formalizes. We encode the $k$-step transition probabilities for a homogeneous Markov chain in a stochastic matrix of dimension $|\Omega| \times |\Omega|$:
	\begin{equation}
		\iterate{P}{k} = [ \iterate{p}{k}_{ij} ] = [ \prob(X_{t+k} = s_j \mid X_t = s_i) ] = [ \prob(X_{t} = s_j \mid X_0 = s_i) ].
	\end{equation}
	Note how the memorylessness of the Markov chain enables the powerful final equality above. By the definition of the transition matrix, we see $ [ \iterate{p}{1}_{ij} ] = P$. Now, suppose inductively that $\iterate{p}{\ell}_{ij} = \parens{\iterate{P}{\ell}}_{ij}$ for all $i, j \in \Omega$. Then, observe the following:
	\begin{align*}
		\iterate{p}{1+\ell}_{ij} &= \prob(X_{\ell+1} = s_j \mid X_0 = s_i) \\
		&= \sum_{s_k\in\Omega}{\prob(X_1 = s_k \mid X_0 = s_i) \prob(X_{1+\ell} = s_j \mid X_1 = s_k)} && \text{(total probability)} \\
		&= \sum_{s_k\in\Omega}{\prob(X_1 = s_k \mid X_0 = s_i) \prob(X_{\ell} = s_j \mid X_0 = s_k)} && \text{(homogeneity)} \\
		&= \sum_{k\leq |\Omega|}{\parens{\iterate{p}{1}_{ik} \iterate{p}{\ell}_{kj}}} && \text{(definition of transition matrix)}\\
		&= p^{1+\ell}_{ij}. && \text{(matrix multiplication)}
	\end{align*}
	From this result, we derive the Chapman-Kolmogorov equations:
	\begin{equation}
	        \iterate{p}{m+n}_{ij} = \sum_{k\leq |\Omega|}{\parens{\iterate{p}{n}_{ik} \iterate{p}{m}_{kj}}}
	\end{equation}
    We will denote the $k$-step transition probability from state $s_i$ to $s_j$ by directly referencing the components of the transition matrix. Since Markov chains are memoryless, we simply take each successive iteration as if it were the first; in this way, we understand that iteration of transitions is given by matrix multiplication in a homogenous chain. Recall that $k$-paths of graphs are also given by the $k^\text{th}$ power of the adjacency matrix. 
    
    With this in mind, consider a distribution $\iterate{v}{\ell}$ over $\Omega$, defined by $\iterate{v}{\ell}_i = \prob(X_\ell = i)$. Then, by the total law of probability, we calculate the following:
    \begin{align*}
        \iterate{v}{\ell}_i &= \sum_{s_k\in\Omega}{\prob(X_0 = s_k) \prob(x_\ell = s_i \mid X_0 = s_k)} \\
        &= \sum_{k\leq |\Omega|}{\parens{\iterate{v}{0}_k \iterate{p}{\ell}_{ki}}}.
    \end{align*}
    From these results, we conclude that $\iterate{v}{\ell} = \iterate{v}{0} \iterate{P}{\ell} = \iterate{v}{0} P^\ell$, for some initial iterate $\iterate{v}{0}$. We have now more clearly justified the construction of equation \eqref{eqn:pi_t}.
    
    \subsection{Stationarity}
    We can take the stochastic process $\{X_t\}_{t \in T}$ underlying a Markov chain as a categorical distribution over $\mathcal{S}$, the state space.\footnote{The categorical (multinoulli) distribution is a generalization of the Bernoulli distribution, where each random variable may take one of $k$ states.} A chain distribution $\pi$ gives a simplex of dimension $|S| - 1$. Choose such a $P$-invariant simplex. By Brouwer's fixed point theorem, within this simplex must be a $P$-invariant distribution, i.e., an $P$-eigenvector.\footnote{Brower's fixed point theorem, which Sperner's lemma implies, leads to an elegeant argument for Perron's theorem of positive matrices. Michael Brin -- Sergey's father -- first advanced such topological arguments for Perron-Frobenius theory in 1993. We encourage the reader to consult \cite{boyleBasicPerronFrobeniusTheory} for more details. \vspace{0.5em} \\  Note further that the simplexical argument for the stationary distribution is interesting but not at all constructive. We can, in fact, construct an exact definition of $\pi$ from the expected number of visits to each state in a positive recurrent chain. See \cite{freedmanConvergenceTheoremFinite} for an excellent summary of these probabalistic arguments. In this paper, we take a linear-algebraic approach to convergence via the Perron-Frobenius theorem. In hindsight, we see that this route of exposition might have been the messier option. However, since Google determines PageRank through power iteration on the transition matrix, our conversation shows more clearly how PageRank works in the real world.} We denote this fixed distribution $\pi$ such that $\pi P = \pi$. That is, $\pi_i = \prob(X_t = s_i)$ for all $t\in T$ over all states. This time independence defines the stationarity of a distribution $\pi$, and it thus gives an eigenequation for spectral analysis. Consider a stationary distribution $\pi$ that is also a limiting distribution, so that we have the following:
	\begin{equation*}
		\pi = \lim_{k\to\infty}{{\iterate{\pi}{k}}} = \lim_{k\to\infty}{\parens{{\iterate{\pi}{0}} P^k}}.
	\end{equation*}
	
	In a Markov chain with a limiting distribution, any initial distribution $\iterate{\pi}{0}$ will eventually converge upon $\pi$. However, a stationary distribution does not itself imply a limiting distribution. For instance, consider the Markov chain given by $P=\begin{bsmallmatrix} 0 & 1 \\ 1 & 0 \end{bsmallmatrix}$. This standard example corresponds to turning a coin over, so the opposite state is reached with certainty at each step. However, the probability of revealing a certain face at a given step depends entirely upon the initial state -- what face of the coin was initially facing up. Thus, this chain does not have a limiting distribution. However, we have an easy stationary distribution: $v = [\frac{1}{2}, \frac{1}{2}]$, corresponding to choosing the initial state by flipping a fair coin. The stationary distribution hides the chain's dependence upon its initial state. However, if we have a biased coin, say one for which $\iterate{v}{0} = [\frac{3}{5}, \frac{4}{5}]$, the transition probabilities at each turn of the coin clearly depend upon the initial state of the coin. We can also show that no limiting distribution exists by examining the transition matrix. As $k\to\infty$, the iterate $\iterate{P}{k}$ swaps certainty of transition between states; it alternates between $P$ and $I$. Thus, the limit of the transition matrix does not exist, and so a limiting distribution does not exist.
	
    The nature of our spectral ranking problem thus becomes stunningly clear, for the time-independent probabilities within $\pi$ yield a unique metric of the importance we have sought to codify. We can predict, from the structure of the Web itself, how much time a random Web surfer will spend on a given page, and we can take these probabilities as evidence for how important a page actually is. 
    
    Analyzing the asymptopic behavior of $P$ will show us when such a limiting distribution exists, and we will thus know when we have a unique solution to the stationary eigenequation. We answer the vital question of when a Markov chain has such distribution by exploring a fundamental theory of dynamical systems.
\end{document}