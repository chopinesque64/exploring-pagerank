\documentclass[../exploring-pagerank.tex]{subfiles}

\begin{document}
    \subsection{Spectral Resolution\protect\footnote{The results in this section summarize the extended treatment of matrix functions and convergence given in \cite{meyerMatrixAnalysisApplied2000}}.}
    Recall that, for a a diagonalizable matrix $A=PD\inverse{P}$, the columns of the similarity transformation $P$ correspond to distinct right eigenvectors of $A$. Consider partitioning $P$ into bases $\beta_{\mathcal{E}_\lambda}$ for distinct right eigenspaces of $A$. Similarly, we partition the rows of $\inverse{P}$ into bases $\gamma_{\mathcal{E}_\lambda}$ for distinct left eigenspaces. Thus, for a matrix $A$ with with spectrum $\sigma$, we have an outer product formulation of the spectral decomposition:
    \begin{equation}
        \label{eqn:spectral_decomposition}
        A = PD\inverse{P} = \sum_{\lambda\in\sigma}{\lambda [\beta_{\mathcal{E}_\lambda}]\transpose{[\gamma_{\mathcal{E}_\lambda}]}}.
    \end{equation}
    
    Note that $E_\lambda = [\beta_{\mathcal{E}_\lambda}]\transpose{[\gamma_{\mathcal{E}_\lambda}]}$ projects onto the eigenspace $\mathcal{E}_\lambda$. In fact, $A$ is diagonalizable if and only if it can be written as a sum of such spectral projectors. Consider $x$ and $\transpose{y}$ as the handed eigenvectors of a simple eigenvalue $\lambda_s$. We see this elegant formula for the corresponding eigenprojection:
    \begin{equation}
        \label{eqn:simple_eigenprojection}
        E_{\lambda_s} = \frac{x \transpose{y}}{\transpose{y} x},
    \end{equation}
    where the inner product in the denominator comes from normalization to ensure projection properties.
    
    \subsection{Matrix Functions}
	Consider generalizing a scalar function $f$ to be a matrix function. As the well-known matrix exponential illustrates, we often do not define matrix functions as scalar functions applied entrywise, for we then might lose the nice properties associated with the scalar functions. Using the Taylor expansion of a sufficiently-differentiable $f$, we first define such a function about a Jordan block $J_k(\lambda)$:
	\begin{equation*}
		f\big( J_k(\lambda) \big)=f(\lambda)I + f'(\lambda)(J_k(\lambda) - \lambda I) + \frac{f'(\lambda)(J_k(\lambda) - \lambda I)^2}{2!} + \cdots.
	\end{equation*}
	However, since $(J_k(\lambda) - \lambda I)$ is nilpotent of order $k$, the Taylor expansion reduces nicely:
	\begin{equation}
	    \label{eqn:f_jordan_block}
		f\big( J_k(\lambda) \big) = \sum_{i=0}^{k - 1}\parens{\frac{f^{[i]}(\lambda)}{i!} (J_k(\lambda) - \lambda I)^i}.
	\end{equation}
	
    Thus, for $f$ with at least $(k - 1)$ derivatives that exist at $\lambda$, we have the definition of a function applied to a Jordan block:
	\begin{equation}\label{eq:f_jordan_block}
		f\big( J_k(\lambda) \big) =
		\begin{bmatrix}
			f(\lambda) & f'(\lambda) & \frac{f''(\lambda)}{2!} & \ldots & \frac{f^{[k - 1]}(\lambda)}{(k-1)!} \\
			0 & f(\lambda) & f'(\lambda) & \ldots & \frac{f^{[k - 2]}(\lambda)}{(k-2)!} \\
			0 & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \ldots & f(\lambda)
		\end{bmatrix}.
	\end{equation}
	
	For a diagonalizable matrix $A$ as above, note that we can we apply Equation \eqref{eqn:spectral_decomposition} to obtain a nice result:
	\begin{align*}
	    f(A) = Pf(D)\inverse{P} = \sum_{\lambda \in \sigma}{f(\lambda) E_\lambda}.
	\end{align*}
	
	We can extend $f$ to a Jordan form $J$ by requiring that $f$ has at least $(\iota - 1)$ derivatives, where $\iota = \iota_\lambda$ is the index of $\lambda$, i.e., the size of the largest $\lambda$-block. Then, for $A$ with spectrum $\sigma$, not necessarily diagonalizable, we have this function definition:
	\begin{equation*}
	    f(A) = P f(J) \inverse{P} = \sum_{\lambda \in \sigma}\parens{ [\beta_{\mathcal{E}_\lambda}]\, f\big( J_{k_\lambda}(\lambda) \big)\, \transpose{[\gamma_{\mathcal{E}_\lambda}]} }.
	\end{equation*}
    Expanding the definition of the function and applying the eigenprojection gives this form:
    \begin{equation}
        f(A) = \sum_{\lambda \in \sigma}\sum_{j=0}^{\iota_{\lambda} - 1}\parens{ \frac{f^{[j]}(\lambda)}{i!} (J_{k_\lambda}(\lambda) - \lambda I)^j E_{\lambda} }.
    \end{equation}
    
    \subsection{Matrix Convergence} 
    Consider the real exponentiation function $f(x) = x^n$ for some $n$. When we apply $f$ to a Jordan block, we give the coefficients of differentiation as binomial coefficients.
	\begin{equation}\label{eq:power_jordan_block}
		\big( J_k(\lambda) \big)^n =
		\begin{bmatrix}
			\lambda^n & \binom{n}{1}(\lambda^{n-1}) & \binom{n}{2}(\lambda^{n-2}) & \ldots & \binom{n}{k-1}(\lambda^{n-(k+1)}) \\
			0 & \lambda^n & \binom{n}{1}(\lambda^{m-1}) & \ldots & \binom{n}{k-2}(\lambda^{m-(k+2)}) \\
			0 & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \ldots & \lambda^n
		\end{bmatrix}.
	\end{equation}

    In general, we define matrix exponentiation thusly:
        \begin{equation}
        A^n = \sum_{\lambda \in \sigma}\sum_{j=0}^{\iota_{\lambda} - 1}\parens{ \binom{n}{j} \lambda^{n-j} (J_{k_\lambda}(\lambda) - \lambda I)^j E_{\lambda} }.
    \end{equation}
    
    Suppose $| \lambda | < 1$. Then, for each $j \in [1, n-(k+1)]$, we have
    \begin{equation*}
        \abs*{\binom{n}{j}\lambda^{m-j}} \leq \frac{k^j}{j!}{\abs*{\lambda^{m-j}}}.
    \end{equation*}
    Note that as $m \to \infty$, the numerator $k^j$ diverges polynomially since $j$ is fixed. However, since $| \lambda | < 1$, $\abs{\lambda^{m-j}}$ converges to zero exponentially. Thus, we conclude the following:
    \begin{equation*}
        \lim_{n\to\infty}{\big( J_k(\lambda) \big)^n} = 0.
    \end{equation*}
    We can extend this argument to show that, for a square matrix $A$, $\lim_{n\to\infty}{\parens{A^n}} = 0$ if and only if $\rho(A) < 1$, and similar reasoning shows the limit does not exist when $\rho(A) > 1$.
    
    We now study the convergence of a matrix with $\rho(A) = 1$, such as we have with all stochastic (and primitive) matrices. Suppose we have a primitive Markov chain with transition matrix $P$. By the Perron-Frobenius theorem, we know $P$ has dominant eigenvalue $\lambda_1 = 1$. As all non-dominant eigenvalues have modulus less than 1 and will thus cause their terms to converge to zero, we simply have
    \begin{equation*}
        \lim_{n\to\infty}{A^n} = E_{\lambda_1}.
    \end{equation*}
    Thus, powers of a primitive matrix will eventually converge upon its dominant eigenprojector. For primitive Markov chains, we have a left eigenvector $\pi$, the stationary distribution. Moreover, we previously showed that all stochastic matrices have right eigenvector $\mathbbm{1}$. Thus, using the limit we just discovered, we have a limiting distribution:
    \begin{equation}
        \lim_{k\to\infty}{{\iterate{\pi}{k}}} = \lim_{k\to\infty}{\parens{{\iterate{\pi}{0}} P^k}} = \iterate{\pi}{0} \parens{\frac{\mathbbm{1} \pi}{\pi \mathbbm{1}}} = \parens{\iterate{\pi}{0} \mathbbm{1}} \pi = \pi. 
    \end{equation}
    Thus, primitivity guarantees a distribution completely independent of the chain's starting state. The convergence of a primitive chain is a rather remarkable result, one often called the fundamental theorem of Markov chains.
    
    \subsection{Power Iteration}
    Applying Perron-Frobenius theory to primitive Markov chains gives a classical method for calculating the limiting distribution. Techniques based on Gaussian elimination would not be a smart choice for calculating a limiting distribution $\pi$, since the algorithm has arithmetic complexity of order $O(n^3)$. Such a slow algorithm could not rank the nearly $10^{11}$ vertices of the indexed Internet. 
    
    Here, we explore the power method -- the simplest iterative calculation method. Suppose a square matrix $A$ has $n$ eigenpairs $\{ (\lambda_i,x_i)_{i\leq n} \}$ with $\lambda_1$ as a dominant eigenvalue. Consider a starting vector $x^{(0)}=\sum_i{\gamma_i x_i}$ for scalars $\gamma_i$. Then, we again consider an iterative calculation:
	\begin{equation}
		\iterate{x}{k} = A^k \iterate{x}{0} = \sum_{i=1}^{k}{\gamma_i \lambda_i^k x_i} = \lambda_1^k\parens{\gamma_1 x_1 + \sum_{i=2}^{k}{\gamma_i \frac{\lambda_i}{\lambda_1} x_i}} \to \lambda_1^k \gamma_1 x_1.
	\end{equation}
    Since $\lambda_i<\lambda_1$ for $i>1$, the sum approaches 0 as $k\to\infty$. The speed of convergence thus depends upon how quickly $\parens{\frac{\lambda_i}{\lambda_1}}^k \to 0$, and specifically upon the subdominant eigenvalue $\lambda_2$. Power method applications often normalize the the estimated eigenvector after each iteration to avoid underflow or overflow under successive powers of the eigenvalue, but we need not worry about this since primitive Markov chains have dominant eigenvalue 1.
\end{document}