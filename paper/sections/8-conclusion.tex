\documentclass[../exploring-pagerank.tex]{subfiles}

\begin{document}
	Since PageRank incorporates the entire Web into its calculation, it is query-independent. The refinements given in \cite{haveliwalaAnalyticalComparisonApproaches2003} adapt the stochasticity and teleportation distributions to provide a personalized PageRank. However, the expense of calculation across the Web means PageRank can only be personalized to a coarse set of categories that with independent biasing distributions. Vector space models or Bayesian classifiers rank a query in these categories, and the resulting PageRanks are weighted accordingly. Such personalization can also be used to defeat link spamming, where businesses with link farms -- clusters of sites with high PageRank -- manipulate the Web structure to bestow undeservingly good ranks upon their clients' pages.
	
	Query-dependent algorithms like HITS beat PageRank in personalized searching, as they run a power method on small components of the Web. However, the more serious induced deficiencies -- like more imprecise ranking and susceptibility to link spamming -- have made PageRank the dominant spectral ranking algorithm on the Web. Moreover, the definition of importance built into PageRank means it can predict backlinks (citations) well; Brin and Page \cite{brinPageRankCitationRanking1998} showed that PageRank determines an optimal path for Web crawlers, who should index the most important parts of the Web quickly and not be trapped by closely-linked local chains.
	
	Modern PageRank research has emphasized efficiently and dynamically calculating perturbations in the PageRank distribution $\pi$, which maps to over $10^{11}$ Web pages. We would like to reduce the number of power iterations and ease the calculations within those iterations. This, in turn, helps us make PageRank more and more query-responsive. The adaptive method given in \cite{berkhinSurveyPageRankComputing2005} notes that the most highly-connected pages in a Web generally take the longest to converge upon their PageRanks. Rather than running a brute power method that recalculates the entire $\pi$ each time, we just freeze the pages that have already converged within some tolerance. More work should be done on determining the convergence properties of this method.
	
	Aitken $\delta^2$ extrapolation, applied to PageRank in \cite{langvilleGooglePageRankScience2006}, estimates the eigenvector of the subdominant eigenvalue and corrects for it. This does not always work, as page clustering also impacts higher-order eigenvectors. Indeed, \cite{kamvarExploitingBlockStructure2003} gives a calculation for PageRank that aims for both objectives by recursively calculating ranks of irreducible Web blocks. In this way, the aggregation method uses the reducibility of the Web to its advantage.
	
    Google still relies heavily upon the traditional vector space relevance models and sophisticated analysis of anchor text (the visible text of a hyperlink), font style, and capitalization. PageRank, after all, addresses a specific problem of structural relevance; other systems must decide the a page's contextual relevance to a query. PageRank, however, forms the central part of Google's broader search strategy -- a system designed to scale even to a Web with $10^{100}$ pages -- a \textit{googol}.
\end{document}